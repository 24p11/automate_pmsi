
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>transformers.modeling_tf_roberta &#8212; tf_codage 0.1 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/css-style.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for transformers.modeling_tf_roberta</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot; TF 2.0 RoBERTa model. &quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="p">(</span><span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span>
                        <span class="n">unicode_literals</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">.configuration_roberta</span> <span class="kn">import</span> <span class="n">RobertaConfig</span>
<span class="kn">from</span> <span class="nn">.modeling_tf_utils</span> <span class="kn">import</span> <span class="n">TFPreTrainedModel</span><span class="p">,</span> <span class="n">get_initializer</span><span class="p">,</span> <span class="n">shape_list</span>
<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">add_start_docstrings</span>

<span class="kn">from</span> <span class="nn">.modeling_tf_bert</span> <span class="kn">import</span> <span class="n">TFBertEmbeddings</span><span class="p">,</span> <span class="n">TFBertMainLayer</span><span class="p">,</span> <span class="n">gelu</span><span class="p">,</span> <span class="n">gelu_new</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;roberta-base&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5&quot;</span><span class="p">,</span>
    <span class="s1">&#39;roberta-large&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5&quot;</span><span class="p">,</span>
    <span class="s1">&#39;roberta-large-mnli&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5&quot;</span><span class="p">,</span>
    <span class="s1">&#39;distilroberta-base&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">TFRobertaEmbeddings</span><span class="p">(</span><span class="n">TFBertEmbeddings</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Applies embedding based on inputs tensor.&quot;&quot;&quot;</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)[</span><span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_embedding</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">],</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TFRobertaMainLayer</span><span class="p">(</span><span class="n">TFBertMainLayer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as TFBertMainLayer but uses TFRobertaEmbeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaMainLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">TFRobertaEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embeddings&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span>


<span class="k">class</span> <span class="nc">TFRobertaPreTrainedModel</span><span class="p">(</span><span class="n">TFPreTrainedModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; An abstract class to handle weights initialization and</span>
<span class="sd">        a simple interface for dowloading and loading pretrained models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">RobertaConfig</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="n">TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;roberta&quot;</span>


<span class="n">ROBERTA_START_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;    The RoBERTa model was proposed in</span>
<span class="s2">    `RoBERTa: A Robustly Optimized BERT Pretraining Approach`_</span>
<span class="s2">    by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,</span>
<span class="s2">    Veselin Stoyanov. It is based on Google&#39;s BERT model released in 2018.</span>
<span class="s2">    </span>
<span class="s2">    It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining</span>
<span class="s2">    objective and training with much larger mini-batches and learning rates.</span>
<span class="s2">    </span>
<span class="s2">    This implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained </span>
<span class="s2">    models.</span>

<span class="s2">    This model is a tf.keras.Model `tf.keras.Model`_ sub-class. Use it as a regular TF 2.0 Keras Model and</span>
<span class="s2">    refer to the TF 2.0 documentation for all matter related to general usage and behavior.</span>

<span class="s2">    .. _`RoBERTa: A Robustly Optimized BERT Pretraining Approach`:</span>
<span class="s2">        https://arxiv.org/abs/1907.11692</span>

<span class="s2">    .. _`tf.keras.Model`:</span>
<span class="s2">        https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model</span>

<span class="s2">    Note on the model inputs:</span>
<span class="s2">        TF 2.0 models accepts two formats as inputs:</span>

<span class="s2">            - having all inputs as keyword arguments (like PyTorch models), or</span>
<span class="s2">            - having all inputs as a list, tuple or dict in the first positional arguments.</span>

<span class="s2">        This second option is usefull when using `tf.keras.Model.fit()` method which currently requires having all the tensors in the first argument of the model call function: `model(inputs)`.</span>

<span class="s2">        If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the first positional argument :</span>

<span class="s2">        - a single Tensor with input_ids only and nothing else: `model(inputs_ids)</span>
<span class="s2">        - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:</span>
<span class="s2">            `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`</span>
<span class="s2">        - a dictionary with one or several input Tensors associaed to the input names given in the docstring:</span>
<span class="s2">            `model({&#39;input_ids&#39;: input_ids, &#39;token_type_ids&#39;: token_type_ids})`</span>

<span class="s2">    Parameters:</span>
<span class="s2">        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the </span>
<span class="s2">            model. Initializing with a config file does not load the weights associated with the model, only the configuration.</span>
<span class="s2">            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">ROBERTA_INPUTS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Inputs:</span>
<span class="s2">        **input_ids**: ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length)``:</span>
<span class="s2">            Indices of input sequence tokens in the vocabulary.</span>
<span class="s2">            To match pre-training, RoBERTa input sequence should be formatted with &lt;s&gt; and &lt;/s&gt; tokens as follows:</span>

<span class="s2">            (a) For sequence pairs:</span>

<span class="s2">                ``tokens:         &lt;s&gt; Is this Jacksonville ? &lt;/s&gt; &lt;/s&gt; No it is not . &lt;/s&gt;``</span>

<span class="s2">            (b) For single sequences:</span>

<span class="s2">                ``tokens:         &lt;s&gt; the dog is hairy . &lt;/s&gt;``</span>

<span class="s2">            Fully encoded sequences or sequence pairs can be obtained using the RobertaTokenizer.encode function with </span>
<span class="s2">            the ``add_special_tokens`` parameter set to ``True``.</span>

<span class="s2">            RoBERTa is a model with absolute position embeddings so it&#39;s usually advised to pad the inputs on</span>
<span class="s2">            the right rather than the left.</span>

<span class="s2">            See :func:`transformers.PreTrainedTokenizer.encode` and</span>
<span class="s2">            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.</span>
<span class="s2">        **attention_mask**: (`optional`) ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length)``:</span>
<span class="s2">            Mask to avoid performing attention on padding token indices.</span>
<span class="s2">            Mask values selected in ``[0, 1]``:</span>
<span class="s2">            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.</span>
<span class="s2">        **token_type_ids**: (`optional` need to be trained) ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length)``:</span>
<span class="s2">            Optional segment token indices to indicate first and second portions of the inputs.</span>
<span class="s2">            This embedding matrice is not trained (not pretrained during RoBERTa pretraining), you will have to train it</span>
<span class="s2">            during finetuning.</span>
<span class="s2">            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``</span>
<span class="s2">            corresponds to a `sentence B` token</span>
<span class="s2">            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).</span>
<span class="s2">        **position_ids**: (`optional`) ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length)``:</span>
<span class="s2">            Indices of positions of each input sequence tokens in the position embeddings.</span>
<span class="s2">            Selected in the range ``[0, config.max_position_embeddings - 1[``.</span>
<span class="s2">        **head_mask**: (`optional`) ``Numpy array`` or ``tf.Tensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:</span>
<span class="s2">            Mask to nullify selected heads of the self-attention modules.</span>
<span class="s2">            Mask values selected in ``[0, 1]``:</span>
<span class="s2">            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.</span>
<span class="s2">        **inputs_embeds**: (`optional`) ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length, embedding_dim)``:</span>
<span class="s2">            Optionally, instead of passing ``input_ids`` you can choose to directly pass an embedded representation.</span>
<span class="s2">            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class="s2">            than the model&#39;s internal embedding lookup matrix.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;The bare RoBERTa Model transformer outputing raw hidden-states without any specific head on top.&quot;</span><span class="p">,</span>
                      <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span> <span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TFRobertaModel</span><span class="p">(</span><span class="n">TFRobertaPreTrainedModel</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:</span>
<span class="sd">        **last_hidden_state**: ``tf.Tensor`` of shape ``(batch_size, sequence_length, hidden_size)``</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the model.</span>
<span class="sd">        **pooler_output**: ``tf.Tensor`` of shape ``(batch_size, hidden_size)``</span>
<span class="sd">            Last layer hidden-state of the first token of the sequence (classification token)</span>
<span class="sd">            further processed by a Linear layer and a Tanh activation function. The Linear</span>
<span class="sd">            layer weights are trained from the next sentence prediction (classification)</span>
<span class="sd">            objective during Bert pretraining. This output is usually *not* a good summary</span>
<span class="sd">            of the semantic content of the input, you&#39;re often better with averaging or pooling</span>
<span class="sd">            the sequence of hidden-states for the whole input sequence.</span>
<span class="sd">        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for the output of each layer + the output of the embeddings)</span>
<span class="sd">            of shape ``(batch_size, sequence_length, hidden_size)``:</span>
<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        **attentions**: (`optional`, returned when ``config.output_attentions=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:</span>
<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from transformers import RobertaTokenizer, TFRobertaModel</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = TFRobertaModel.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;))[None, :]  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids)</span>
<span class="sd">        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">TFRobertaMainLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;roberta&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">TFRobertaLMHead</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Roberta Head for masked language modeling.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">input_embeddings</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaLMHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span>
                                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="n">gelu</span><span class="p">)</span>

        <span class="c1"># The output weights are the same as the input embeddings, but there is</span>
        <span class="c1"># an output-only bias for each token.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">input_embeddings</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,),</span>
                                    <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                                    <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaLMHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># project back to size of vocabulary with bias</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;RoBERTa Model with a `language modeling` head on top. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span> <span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TFRobertaForMaskedLM</span><span class="p">(</span><span class="n">TFRobertaPreTrainedModel</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        **masked_lm_labels**: (`optional`) ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length)``:</span>
<span class="sd">            Labels for computing the masked language modeling loss.</span>
<span class="sd">            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)</span>
<span class="sd">            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels</span>
<span class="sd">            in ``[0, ..., config.vocab_size]``</span>

<span class="sd">    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:</span>
<span class="sd">        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``tf.Tensor`` of shape ``(1,)``:</span>
<span class="sd">            Masked language modeling loss.</span>
<span class="sd">        **prediction_scores**: ``tf.Tensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``</span>
<span class="sd">            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</span>
<span class="sd">        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for the output of each layer + the output of the embeddings)</span>
<span class="sd">            of shape ``(batch_size, sequence_length, hidden_size)``:</span>
<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        **attentions**: (`optional`, returned when ``config.output_attentions=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:</span>
<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from transformers import RobertaTokenizer, TFRobertaForMaskedLM</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = TFRobertaForMaskedLM.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;))[None, :]  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids, masked_lm_labels=input_ids)</span>
<span class="sd">        prediction_scores = outputs[0]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaForMaskedLM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">TFRobertaMainLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;roberta&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">TFRobertaLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lm_head&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prediction_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction_scores</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># Add hidden states and attention if they are here</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># prediction_scores, (hidden_states), (attentions)</span>


<span class="k">class</span> <span class="nc">TFRobertaClassificationHead</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Head for sentence-level classification tasks.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaClassificationHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span>
                                           <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                                           <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">,</span>
                                              <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span>
                                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;out_proj&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># take &lt;s&gt; token (equiv. to [CLS])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer </span>
<span class="s2">    on top of the pooled output) e.g. for GLUE tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span> <span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TFRobertaForSequenceClassification</span><span class="p">(</span><span class="n">TFRobertaPreTrainedModel</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:</span>
<span class="sd">        **logits**: ``tf.Tensor`` of shape ``(batch_size, config.num_labels)``</span>
<span class="sd">            Classification (or regression if config.num_labels==1) scores (before SoftMax).</span>
<span class="sd">        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for the output of each layer + the output of the embeddings)</span>
<span class="sd">            of shape ``(batch_size, sequence_length, hidden_size)``:</span>
<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        **attentions**: (`optional`, returned when ``config.output_attentions=True``)</span>
<span class="sd">            list of ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:</span>
<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from transformers import RobertaTokenizer, TFRobertaForSequenceClassification</span>

<span class="sd">        tokenizer = RoertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = TFRobertaForSequenceClassification.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;))[None, :]  # Batch size 1</span>
<span class="sd">        labels = tf.constant([1])[None, :]  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids)</span>
<span class="sd">        logits = outputs[0]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaForSequenceClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">TFRobertaMainLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;roberta&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">TFRobertaClassificationHead</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;classifier&quot;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># logits, (hidden_states), (attentions)</span>


<span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;RoBERTa Model with a token classification head on top (a linear layer on top of</span>
<span class="s2">    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">ROBERTA_START_DOCSTRING</span><span class="p">,</span> <span class="n">ROBERTA_INPUTS_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TFRobertaForTokenClassification</span><span class="p">(</span><span class="n">TFRobertaPreTrainedModel</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:</span>
<span class="sd">        **scores**: ``Numpy array`` or ``tf.Tensor`` of shape ``(batch_size, sequence_length, config.num_labels)``</span>
<span class="sd">            Classification scores (before SoftMax).</span>
<span class="sd">        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)</span>
<span class="sd">            list of ``Numpy array`` or ``tf.Tensor`` (one for the output of each layer + the output of the embeddings)</span>
<span class="sd">            of shape ``(batch_size, sequence_length, hidden_size)``:</span>
<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        **attentions**: (`optional`, returned when ``config.output_attentions=True``)</span>
<span class="sd">            list of ``Numpy array`` or ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:</span>
<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.</span>

<span class="sd">    Examples::</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        from transformers import RobertaTokenizer, TFRobertaForTokenClassification</span>

<span class="sd">        tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        model = TFRobertaForTokenClassification.from_pretrained(&#39;roberta-base&#39;)</span>
<span class="sd">        input_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;, add_special_tokens=True))[None, :]  # Batch size 1</span>
<span class="sd">        outputs = model(input_ids)</span>
<span class="sd">        scores = outputs[0]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TFRobertaForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">TFRobertaMainLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;roberta&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">,</span>
                                                <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">get_initializer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">),</span>
                                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;classifier&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># add hidden states and attention if they are here</span>

        <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># scores, (hidden_states), (attentions)</span>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">tf_codage</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cli.html">Command line tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html"><code class="docutils literal notranslate"><span class="pre">tf_codage</span></code> API</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Bartosz Telenczuk, Rémi Flicoteaux.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>